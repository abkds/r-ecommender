{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import csv \n",
    "import scipy\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit import alternating_least_squares\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('interactions_30_ch_no_bots') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=' ')\n",
    "    for subreddit, user, comments, _ in datareader:\n",
    "        data.append([user, subreddit, int(comments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['user', 'subreddit', 'comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['subreddit'] = data['subreddit'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sparse matrix of all the artist/user/play triples\n",
    "comments = coo_matrix((data['comments'].astype(float), \n",
    "                   (data['subreddit'].cat.codes, \n",
    "                    data['user'].cat.codes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle this variable if you want to recalculate the als factors\n",
    "read_als_factors_from_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_als_factors_from_file:\n",
    "    subreddit_factors = np.load('subreddit_factors_als.npy')\n",
    "    user_factors = np.load('user_factors_als.npy')\n",
    "else:\n",
    "    subreddit_factors, user_factors = alternating_least_squares(bm25_weight(comments), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:implicit:This method is deprecated. Please use the AlternatingLeastSquares class instead\n",
      "100%|██████████| 15.0/15 [00:56<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "subreddit_factors, user_factors = alternating_least_squares(bm25_weight(comments), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopRelated(object):\n",
    "    def __init__(self, subreddit_factors):\n",
    "        norms = np.linalg.norm(subreddit_factors, axis=-1)\n",
    "        self.factors = subreddit_factors / norms[:, np.newaxis]\n",
    "        self.subreddits = data['subreddit'].cat.categories.array.to_numpy()\n",
    "\n",
    "    def get_related(self, subreddit, N=10):\n",
    "        subredditid = np.where(self.subreddits == subreddit)[0][0]\n",
    "        scores = self.factors.dot(self.factors[subredditid])\n",
    "        best = np.argpartition(scores, -N)[-N:]\n",
    "        best_ = [self.subreddits[i] for i in best]\n",
    "        return sorted(zip(best_, scores[best]), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_related = TopRelated(subreddit_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OnePiece', 0.99999994),\n",
       " ('Naruto', 0.99061096),\n",
       " ('bleach', 0.98533773),\n",
       " ('OnePunchMan', 0.98216236),\n",
       " ('gamingadvice', 0.9787289),\n",
       " ('KillLaKill', 0.9785078),\n",
       " ('animebazaar', 0.97804713),\n",
       " ('StardustCrusaders', 0.9744566),\n",
       " ('snowleopards', 0.97381645),\n",
       " ('RolledTheDice', 0.97334623)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_related.get_related('OnePiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41143, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41143, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits_embedded = umap.UMAP().fit_transform(subreddit_factors)\n",
    "subreddits_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.4981177, -2.786402 ],\n",
       "       [ 4.1943135,  5.09649  ],\n",
       "       [ 4.831988 ,  1.9612825],\n",
       "       ...,\n",
       "       [ 2.7854004,  6.819096 ],\n",
       "       [-1.8735471, -3.6882179],\n",
       "       [ 5.0341783,  3.1146567]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = data['subreddit'].cat.categories.array.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "indices = random.sample(range(len(subreddits)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_subreddits = subreddits[indices]\n",
    "sampled_subreddits_embedded = subreddits_embedded[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~abkds/0 or inside your plot.ly account where it is named 'basic-scatter'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~abkds/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username='abkds', api_key='KKuXHMUKu7EHg9kIZWrl')\n",
    "\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "N = 500\n",
    "xs = sampled_subreddits_embedded[:, 0]\n",
    "ys = sampled_subreddits_embedded[:, 1]\n",
    "\n",
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    x = xs,\n",
    "    y = ys,\n",
    "    mode='markers+text',\n",
    "    text=sampled_subreddits\n",
    ")\n",
    "\n",
    "data_ = [trace]\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(data_, filename='basic-scatter')\n",
    "\n",
    "# or plot with: plot_url = py.plot(data, filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Personalized Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "\n",
    "params = {\"factors\": 63}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianPersonalizedRanking(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bpr'\n",
    "output_filename = 'subreddits_recs_bpr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:57<00:00,  2.68s/it, correct=95.88%, skipped=7.16%]\n"
     ]
    }
   ],
   "source": [
    "model.fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_related_subreddits(subreddit):\n",
    "    found = np.where(subreddits == subreddit)\n",
    "    if len(found[0]) == 0:\n",
    "        raise ValueError(\"Subreddit doesn't exist in the dataset.\")\n",
    "    _id = found[0][0]\n",
    "    return [(subreddits[i], v) for i, v in model.similar_items(_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 2.9314072),\n",
       " ('Dogtraining', 2.7764423),\n",
       " ('puppy101', 2.6298523),\n",
       " ('Pets', 2.5438745),\n",
       " ('AskVet', 2.5281637),\n",
       " ('rescuedogs', 2.4704523),\n",
       " ('WiggleButts', 2.4026453),\n",
       " ('DoggyDNA', 2.3814583),\n",
       " ('mutt', 2.3722208),\n",
       " ('schnauzers', 2.3701365)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_related_subreddits('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data['user'].cat.categories.array.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_bpr_recommendations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comments = comments.T.tocsr()\n",
    "if write_bpr_recommendations:\n",
    "    # generate recommendations for each user and write out to a file\n",
    "    with tqdm.tqdm_notebook(total=len(users)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for userid, username in enumerate(users):\n",
    "                for subredditid, score in model.recommend(userid, user_comments):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (username, subreddits[subredditid], score))\n",
    "                progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample user recommendations\n",
    "\n",
    "We went through the user 'xkcd_transciber' list of subreddits, where he/she commented. Taking a view of the kind of subreddits followed by the user we see that the predictions are good. This is just one sample, we are saving the recommendations for all users in a file and will also write the AUC score function for getting the exact scores for the generated recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(username):\n",
    "    sample_user_id = np.where(users == username)[0][0]\n",
    "    return [(subreddits[i], v) for i, v in model.recommend(2293528, user_comments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programmingcirclejerk', 3.6349902),\n",
       " ('CivCringe', 3.4902062),\n",
       " ('LinuxCirclejerk', 3.413476),\n",
       " ('cataclysmdda', 3.3854566),\n",
       " ('roguelikedev', 3.3806605),\n",
       " ('spam', 3.3729367),\n",
       " ('AutoModerator', 3.3288083),\n",
       " ('modclub', 3.3230174),\n",
       " ('unfilter', 3.2809386),\n",
       " ('Cyberpunk_Music', 3.2804885)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_for_user('xkcd_transcriber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddits_interacted_by_user(username):\n",
    "    sample_user_id = np.where(users == username)[0][0]\n",
    "    _idlist =  comments.getcol(sample_user_id)\n",
    "    return [subreddits[idx] for idx, i in enumerate(_idlist.toarray()) if i != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Austin',\n",
       " 'BlackPeopleTwitter',\n",
       " 'videos',\n",
       " 'Planetside',\n",
       " 'roosterteeth',\n",
       " 'mistyfront',\n",
       " 'oculus',\n",
       " 'TrollXGirlGamers',\n",
       " 'DIY',\n",
       " 'speedrun',\n",
       " 'houston',\n",
       " 'wow',\n",
       " 'chadev',\n",
       " 'skyrim',\n",
       " 'AskFeminists',\n",
       " 'brasil',\n",
       " 'firefly',\n",
       " 'asktransgender',\n",
       " 'Ubuntu',\n",
       " 'clickbaitEd',\n",
       " 'ANormalDayInRussia',\n",
       " 'gamedev',\n",
       " 'FloridaMan',\n",
       " 'javascript',\n",
       " 'rva',\n",
       " 'sex',\n",
       " 'CasualConversation',\n",
       " 'moraldilemmas',\n",
       " 'me_irl',\n",
       " 'NoStupidQuestions',\n",
       " 'trackers',\n",
       " 'NoFap',\n",
       " 'ynab',\n",
       " 'Multicopter',\n",
       " 'ProtectAndServe',\n",
       " 'geography',\n",
       " 'sciencefiction',\n",
       " 'rccars',\n",
       " 'InternetIsBeautiful',\n",
       " 'arabs',\n",
       " 'Frozen',\n",
       " 'economy',\n",
       " 'crypto',\n",
       " 'techsupportgore',\n",
       " 'latterdaysaints',\n",
       " 'lgbt',\n",
       " 'asoiaf',\n",
       " 'collapse',\n",
       " 'makeyourchoice',\n",
       " 'ar15']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 50 reddits with which xkcd_transcriber has interacted with.\n",
    "random.sample(subreddits_interacted_by_user('xkcd_transcriber'), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1188b1b729324d5cbc1874794e2b7aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1225a3506f834839a37a47450d84b53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2323019), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set seed to get the same train and test set\n",
    "np.random.seed(42)\n",
    "\n",
    "filename = 'interactions_30_ch_no_bots'\n",
    "train_filename = 'interactions_5'\n",
    "\n",
    "def create_dataset():\n",
    "    data = defaultdict(lambda: [])\n",
    "    with open(filename) as csvfile:\n",
    "        datareader = csv.reader(csvfile, delimiter=' ')        \n",
    "        for subreddit, user, comments, _ in tqdm.tqdm_notebook(datareader):\n",
    "            data[user].append((subreddit, comments))\n",
    "    \n",
    "\n",
    "    f_train = open(train_filename, 'a')\n",
    "    \n",
    "    for user, items in tqdm.tqdm_notebook(data.items()):\n",
    "        np.random.shuffle(items)\n",
    "        if len(items) >= 5:\n",
    "            for item in items:\n",
    "                line = ' '.join(list(map(str, [item[0], user, item[1]]))) + '\\n'\n",
    "                f_train.write(line)\n",
    "    \n",
    "    f_train.close()\n",
    "        \n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('interactions_5') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=' ')\n",
    "    for subreddit, user, comments in datareader:\n",
    "        data.append([user, subreddit, int(comments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_records(data)\n",
    "data.columns = ['user', 'subreddit', 'comments']\n",
    "\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['subreddit'] = data['subreddit'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sparse matrix of all the artist/user/play triples\n",
    "comments = coo_matrix((data['comments'].astype(float), \n",
    "                   (data['subreddit'].cat.codes, \n",
    "                    data['user'].cat.codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<35276x517371 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5731059 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = data['subreddit'].cat.categories.array.to_numpy()\n",
    "users = data['user'].cat.categories.array.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users for BPR model: 517371\n",
      "Number of subreddits for BPR model: 35276\n"
     ]
    }
   ],
   "source": [
    "print('Number of users for BPR model: %s' % len(users))\n",
    "print('Number of subreddits for BPR model: %s' % len(subreddits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index and the reverse index for the users and subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_to_index(things):\n",
    "    index = {}\n",
    "    for idx, item in enumerate(things):\n",
    "        index[item] = idx\n",
    "    return index\n",
    "\n",
    "def index_to_item(index):\n",
    "    things = np.empty(len(index), dtype=object)\n",
    "    for item, idx in index.items():\n",
    "        things[idx] = item\n",
    "    return things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_index = item_to_index(subreddits)\n",
    "users_index = item_to_index(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting test set\n",
    "\n",
    "We will pluck out the test set, as per the strategy given in the paper [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/pdf/1205.2618.pdf), section 6.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(coo_comments):\n",
    "    \"\"\"\n",
    "    Omits random user subreddit interactions, zeros them out \n",
    "    and appends them to the test list.\n",
    "    \"\"\"\n",
    "    csr_comments = coo_comments.tocsr()\n",
    "    \n",
    "    data = defaultdict(lambda: [])\n",
    "    with open('interactions_5') as csvfile:\n",
    "        datareader = csv.reader(csvfile, delimiter=' ')        \n",
    "        for subreddit, user, comments in tqdm.tqdm_notebook(datareader):\n",
    "            data[user].append((subreddit, comments))\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for user, items in tqdm.tqdm_notebook(data.items()):\n",
    "        np.random.shuffle(items)\n",
    "        test_item = items[0]\n",
    "        test_comments = items[1]\n",
    "        \n",
    "        test_subreddit = test_item[0]\n",
    "        # zero out a user item interaction\n",
    "        csr_comments[subreddits_index[test_subreddit], users_index[user]] = 0\n",
    "        \n",
    "        test_set.append([test_subreddit, user, int(comments)])\n",
    "        \n",
    "        for item in items[1:]:\n",
    "            train_set.append([item[0], user, int(item[1])])\n",
    "        \n",
    "    csr_comments.eliminate_zeros()\n",
    "    return train_set, test_set, csr_comments.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239592f849554cf89b163b53df5757d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca9200f3c524cf6a736dad6b0774f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=517371), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set, test_set, comments = train_test_split(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC Metric\n",
    "\n",
    "We will implement the AUC Metric for evaluation of BPR based methods. We take the definition given in the paper [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/pdf/1205.2618.pdf), section 6.2 . AUC is defined as \n",
    " \n",
    "$$AUC = \\frac{1}{| U |} \\sum_u \\frac{1}{|E(u)|} \\sum_{(i, j) \\in E(u)} \\delta(\\hat{x}_{ui} - \\hat{x}_{xj}) $$\n",
    "\n",
    "where $$E(u) := \\{(i, j) | (u, i) \\in S_{test} ∧ (u, j) \\notin (S_{test} ∪ S_{train})\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a46d2fedb5045628884af5bcbd93095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5213688), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cc353c99584852a69a181bf6bf0412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=517371), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create E(u) list for each user and store it use ids instead of names to store them\n",
    "E_u = defaultdict(lambda : set())\n",
    "\n",
    "for subreddit, user, _ in tqdm.tqdm_notebook(train_set):\n",
    "        E_u[users_index[user]].add(subreddits_index[subreddit])\n",
    "            \n",
    "for subreddit, user, _ in tqdm.tqdm_notebook(test_set):\n",
    "        E_u[users_index[user]].add(subreddits_index[subreddit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the bpr model \n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "\n",
    "params = {\"factors\": 63}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianPersonalizedRanking(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<35276x517371 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5213688 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:18<00:00,  1.29s/it, correct=94.68%, skipped=9.65%]\n"
     ]
    }
   ],
   "source": [
    "model.fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subreddits = len(subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(test_set, user_factors, subreddit_factors, subreddits, users):\n",
    "    \"\"\"\n",
    "    Returns the auc score on a test data set\n",
    "    \"\"\"\n",
    "    num_users = len(test_set)\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    # treat the signal as 1 as per the implicit bpr paper\n",
    "    for subreddit, user, signal in tqdm.tqdm_notebook(test_set):  # outer summation\n",
    "        # inner summation \n",
    "        # TODO: try to parallelize \n",
    "        u = users_index[user]\n",
    "        i = subreddits_index[subreddit]\n",
    "        \n",
    "        x_ui = user_factors[u].dot(subreddit_factors[i])\n",
    "        \n",
    "        js = []\n",
    "        \n",
    "        for j in range(0, num_subreddits):\n",
    "            if j != i and j not in E_u[u]:\n",
    "                js.append(j)\n",
    "                \n",
    "        total += np.sum(np.heaviside(x_ui - user_factors[u].dot(subreddit_factors[js].T), 0)) / len(js)\n",
    "            \n",
    "        # for j in range(0, subreddits):\n",
    "        #    numel = 0\n",
    "        #    total_user = 0\n",
    "        #    if j != i and j not in E_u[u]:\n",
    "        #        numel += 1\n",
    "        #        x_uj = user_factors[u].dot(subreddit_factors[j])\n",
    "        #            total_user += heaviside(x_ui - x_uj)\n",
    "        \n",
    "        # total += (total_user * 1.0 / numel)\n",
    "    \n",
    "    return total / num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059ad0e5cc2e421cb160064e53498d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8099910909749478"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(test_set[:10000], model.user_factors, model.item_factors, subreddits, users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
